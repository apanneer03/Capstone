{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8UjTxLbOedH"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Fake News Detection Analysis\n",
        "The Doc2Vec pre-processing\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from gensim import utils\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "def textClean(text):\n",
        "    \"\"\"\n",
        "    Get rid of the non-letter and non-number characters\n",
        "    \"\"\"\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = text.lower().split()\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    text = [w for w in text if not w in stops]\n",
        "    text = \" \".join(text)\n",
        "    return (text)\n",
        "\n",
        "\n",
        "def cleanup(text):\n",
        "    text = textClean(text)\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    return text\n",
        "\n",
        "\n",
        "def constructLabeledSentences(data):\n",
        "    sentences = []\n",
        "    for index, row in data.iteritems():\n",
        "        sentences.append(LabeledSentence(utils.to_unicode(row).split(), ['Text' + '_%s' % str(index)]))\n",
        "    return sentences\n",
        "\n",
        "import pandas as pd\n",
        "def getEmbeddings(path,vector_dimension=300):\n",
        "    \"\"\"\n",
        "    Generate Doc2Vec training and testing data\n",
        "    \"\"\"\n",
        "    data = pd.read_csv(path)\n",
        "\n",
        "    missing_rows = []\n",
        "    for i in range(len(data)):\n",
        "        if data.loc[i, 'text'] != data.loc[i, 'text']:\n",
        "            missing_rows.append(i)\n",
        "    data = data.drop(missing_rows).reset_index().drop(['index','id'],axis=1)\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        data.loc[i, 'text'] = cleanup(data.loc[i,'text'])\n",
        "\n",
        "    x = constructLabeledSentences(data['text'])\n",
        "    y = data['label'].values\n",
        "\n",
        "    text_model = Doc2Vec(min_count=1, window=5, vector_size=vector_dimension, sample=1e-4, negative=5, workers=7, epochs=10,\n",
        "                         seed=1)\n",
        "    text_model.build_vocab(x)\n",
        "    text_model.train(x, total_examples=text_model.corpus_count, epochs=text_model.iter)\n",
        "\n",
        "    train_size = int(0.8 * len(x))\n",
        "    test_size = len(x) - train_size\n",
        "\n",
        "    text_train_arrays = np.zeros((train_size, vector_dimension))\n",
        "    text_test_arrays = np.zeros((test_size, vector_dimension))\n",
        "    train_labels = np.zeros(train_size)\n",
        "    test_labels = np.zeros(test_size)\n",
        "\n",
        "    for i in range(train_size):\n",
        "        text_train_arrays[i] = text_model.docvecs['Text_' + str(i)]\n",
        "        train_labels[i] = y[i]\n",
        "\n",
        "    j = 0\n",
        "    for i in range(train_size, train_size + test_size):\n",
        "        text_test_arrays[j] = text_model.docvecs['Text_' + str(i)]\n",
        "        test_labels[j] = y[i]\n",
        "        j = j + 1\n",
        "\n",
        "    return text_train_arrays, text_test_arrays, train_labels, test_labels\n",
        "\n",
        "\n",
        "def clean_data():\n",
        "    \"\"\"\n",
        "    Generate processed string\n",
        "    \"\"\"\n",
        "    path = 'C:/Users/abinaya/OneDrive/Documents/train.csv'\n",
        "    vector_dimension=300\n",
        "\n",
        "    data = pd.read_csv(path)\n",
        "\n",
        "    missing_rows = []\n",
        "    for i in range(len(data)):\n",
        "        if data.loc[i, 'text'] != data.loc[i, 'text']:\n",
        "            missing_rows.append(i)\n",
        "    data = data.drop(missing_rows).reset_index().drop(['index','id'],axis=1)\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        data.loc[i, 'text'] = cleanup(data.loc[i,'text'])\n",
        "\n",
        "    data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    x = data.loc[:,'text'].values\n",
        "    y = data.loc[:,'label'].values\n",
        "\n",
        "    train_size = int(0.8 * len(y))\n",
        "    test_size = len(x) - train_size\n",
        "\n",
        "    xtr = x[:train_size]\n",
        "    xte = x[train_size:]\n",
        "    ytr = y[:train_size]\n",
        "    yte = y[train_size:]\n",
        "\n",
        "    np.save('xtr_shuffled.npy',xtr)\n",
        "    np.save('xte_shuffled.npy',xte)\n",
        "    np.save('ytr_shuffled.npy',ytr)\n",
        "    np.save('yte_shuffled.npy',yte)"
      ]
    }
  ]
}