{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**UNMASKING DECEPTION: FAKE DETECTION USING NEWS ARTICLE ANAYSIS**"
      ],
      "metadata": {
        "id": "AdOMEMdA5i7x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-4pDJ6SULcD"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Fake News Detection Analysis\n",
        "The Doc2Vec pre-processing\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from gensim import utils\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "def textClean(text):\n",
        "    \"\"\"\n",
        "    Get rid of the non-letter and non-number characters\n",
        "    \"\"\"\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = text.lower().split()\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    text = [w for w in text if not w in stops]\n",
        "    text = \" \".join(text)\n",
        "    return (text)\n",
        "\n",
        "\n",
        "def cleanup(text):\n",
        "    text = textClean(text)\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    return text\n",
        "\n",
        "\n",
        "def constructLabeledSentences(data):\n",
        "    sentences = []\n",
        "    for index, row in data.iteritems():\n",
        "        sentences.append(LabeledSentence(utils.to_unicode(row).split(), ['Text' + '_%s' % str(index)]))\n",
        "    return sentences\n",
        "\n",
        "import pandas as pd\n",
        "def getEmbeddings(path,vector_dimension=300):\n",
        "    \"\"\"\n",
        "    Generate Doc2Vec training and testing data\n",
        "    \"\"\"\n",
        "    data = pd.read_csv(path)\n",
        "\n",
        "    missing_rows = []\n",
        "    for i in range(len(data)):\n",
        "        if data.loc[i, 'text'] != data.loc[i, 'text']:\n",
        "            missing_rows.append(i)\n",
        "    data = data.drop(missing_rows).reset_index().drop(['index','id'],axis=1)\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        data.loc[i, 'text'] = cleanup(data.loc[i,'text'])\n",
        "\n",
        "    x = constructLabeledSentences(data['text'])\n",
        "    y = data['label'].values\n",
        "\n",
        "    text_model = Doc2Vec(min_count=1, window=5, vector_size=vector_dimension, sample=1e-4, negative=5, workers=7, epochs=10,\n",
        "                         seed=1)\n",
        "    text_model.build_vocab(x)\n",
        "    text_model.train(x, total_examples=text_model.corpus_count, epochs=text_model.iter)\n",
        "\n",
        "    train_size = int(0.8 * len(x))\n",
        "    test_size = len(x) - train_size\n",
        "\n",
        "    text_train_arrays = np.zeros((train_size, vector_dimension))\n",
        "    text_test_arrays = np.zeros((test_size, vector_dimension))\n",
        "    train_labels = np.zeros(train_size)\n",
        "    test_labels = np.zeros(test_size)\n",
        "\n",
        "    for i in range(train_size):\n",
        "        text_train_arrays[i] = text_model.docvecs['Text_' + str(i)]\n",
        "        train_labels[i] = y[i]\n",
        "\n",
        "    j = 0\n",
        "    for i in range(train_size, train_size + test_size):\n",
        "        text_test_arrays[j] = text_model.docvecs['Text_' + str(i)]\n",
        "        test_labels[j] = y[i]\n",
        "        j = j + 1\n",
        "\n",
        "    return text_train_arrays, text_test_arrays, train_labels, test_labels\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data():\n",
        "    \"\"\"\n",
        "    Generate processed string\n",
        "    \"\"\"\n",
        "    path = 'C:/Users/abinaya/OneDrive/Documents/train.csv'\n",
        "    vector_dimension=300\n",
        "\n",
        "    data = pd.read_csv(path)\n",
        "\n",
        "    missing_rows = []\n",
        "    for i in range(len(data)):\n",
        "        if data.loc[i, 'text'] != data.loc[i, 'text']:\n",
        "            missing_rows.append(i)\n",
        "    data = data.drop(missing_rows).reset_index().drop(['index','id'],axis=1)\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        data.loc[i, 'text'] = cleanup(data.loc[i,'text'])\n",
        "\n",
        "    data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    x = data.loc[:,'text'].values\n",
        "    y = data.loc[:,'label'].values\n",
        "\n",
        "    train_size = int(0.8 * len(y))\n",
        "    test_size = len(x) - train_size\n",
        "\n",
        "    xtr = x[:train_size]\n",
        "    xte = x[train_size:]\n",
        "    ytr = y[:train_size]\n",
        "    yte = y[train_size:]\n",
        "\n",
        "    np.save('xtr_shuffled.npy',xtr)\n",
        "    np.save('xte_shuffled.npy',xte)\n",
        "    np.save('ytr_shuffled.npy',ytr)\n",
        "    np.save('yte_shuffled.npy',yte)\n"
      ],
      "metadata": {
        "id": "c6Oe3rr61gLy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c13c1829-5303-4b8a-8fe6-80db9feae258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current directory: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-plot\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zum9ZW59ossd",
        "outputId": "2085a828-4332-4c67-9359-8532569648e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-plot in /usr/local/lib/python3.10/dist-packages (0.3.7)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from scikit-plot) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from scikit-plot) (1.2.2)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.10/dist-packages (from scikit-plot) (1.10.1)\n",
            "Requirement already satisfied: joblib>=0.10 in /usr/local/lib/python3.10/dist-packages (from scikit-plot) (1.3.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (4.41.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->scikit-plot) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=1.4.0->scikit-plot) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAM7WWslt4DF",
        "outputId": "75aaea10-0b44-417d-e527-6920dfda14bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes Model:"
      ],
      "metadata": {
        "id": "lCCmtqVx5XCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy Of The Model: 75.96%"
      ],
      "metadata": {
        "id": "D0d_4u5V5awp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Fake News Detection Analysis\n",
        "The Naive Bayes Model\n",
        "\"\"\"\n",
        "\n",
        "from getEmbeddings import getEmbeddings\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scikitplot.plotters as skplt\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "def plot_cmat(yte, ypred):\n",
        "    '''Plotting confusion matrix'''\n",
        "    skplt.plot_confusion_matrix(yte,ypred)\n",
        "    plt.show()\n",
        "\n",
        "# Read the data\n",
        "if not os.path.isfile('./xtr.npy') or \\\n",
        "    not os.path.isfile('./xte.npy') or \\\n",
        "    not os.path.isfile('./ytr.npy') or \\\n",
        "    not os.path.isfile('./yte.npy'):\n",
        "    xtr,xte,ytr,yte = getEmbeddings(\"C:/Users/abinaya/OneDrive/Documents/train.csv\")\n",
        "    np.save('./xtr', xtr)\n",
        "    np.save('./xte', xte)\n",
        "    np.save('./ytr', ytr)\n",
        "    np.save('./yte', yte)\n",
        "\n",
        "xtr = np.load('./xtr.npy')\n",
        "xte = np.load('./xte.npy')\n",
        "ytr = np.load('./ytr.npy')\n",
        "yte = np.load('./yte.npy')\n",
        "\n",
        "# Use the built-in Naive Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(xtr,ytr)\n",
        "y_pred = gnb.predict(xte)\n",
        "m = yte.shape[0]\n",
        "n = (yte != y_pred).sum()\n",
        "print(\"Accuracy = \" + format((m-n)/m*100, '.2f') + \"%\")\n",
        "\n",
        "# Draw the confusion matrix\n",
        "plot_cmat(yte, y_pred)"
      ],
      "metadata": {
        "id": "C9Nqc8sFoY-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machine Model:"
      ],
      "metadata": {
        "id": "nDpTXEmm5HJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy Of The Model: 86.87%"
      ],
      "metadata": {
        "id": "eJBbcKcz5OMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Fake News Detection Analysis\n",
        "The Support Vector Machine model\n",
        "\"\"\"\n",
        "\n",
        "from getEmbeddings import getEmbeddings\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import scikitplot.plotters as skplt\n",
        "import os\n",
        "\n",
        "\n",
        "def plot_cmat(yte, ypred):\n",
        "    '''Plotting confusion matrix'''\n",
        "    skplt.plot_confusion_matrix(yte,ypred)\n",
        "    plt.show()\n",
        "\n",
        "# Read the data\n",
        "if not os.path.isfile('./xtr.npy') or \\\n",
        "    not os.path.isfile('./xte.npy') or \\\n",
        "    not os.path.isfile('./ytr.npy') or \\\n",
        "    not os.path.isfile('./yte.npy'):\n",
        "    xtr,xte,ytr,yte = getEmbeddings(\"C:/Users/abinaya/OneDrive/Documents/train.csv\")\n",
        "    np.save('./xtr', xtr)\n",
        "    np.save('./xte', xte)\n",
        "    np.save('./ytr', ytr)\n",
        "    np.save('./yte', yte)\n",
        "\n",
        "xtr = np.load('./xtr.npy')\n",
        "xte = np.load('./xte.npy')\n",
        "ytr = np.load('./ytr.npy')\n",
        "yte = np.load('./yte.npy')\n",
        "\n",
        "# Use the built-in SVM for classification\n",
        "clf = SVC()\n",
        "clf.fit(xtr, ytr)\n",
        "y_pred = clf.predict(xte)\n",
        "m = yte.shape[0]\n",
        "n = (yte != y_pred).sum()\n",
        "print(\"Accuracy = \" + format((m-n)/m*100, '.2f') + \"%\")\n",
        "\n",
        "# Draw the confusion matrix\n",
        "plot_cmat(yte, y_pred)"
      ],
      "metadata": {
        "id": "RW7Qx5bZwGjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Network Model(Keras):"
      ],
      "metadata": {
        "id": "9MlPV1Ki42de"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy Of The Model: 92.68%"
      ],
      "metadata": {
        "id": "8cwNeatj4_R2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Fake News Detection Analysis\n",
        "The Neural Network-Keras\n",
        "\"\"\"\n",
        "\n",
        "from getEmbeddings import getEmbeddings\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM, Embedding, Input, RepeatVector\n",
        "from keras.optimizers import SGD\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import scikitplot.plotters as skplt\n",
        "import os\n",
        "\n",
        "\n",
        "def plot_cmat(yte, ypred):\n",
        "    '''Plotting confusion matrix'''\n",
        "    skplt.plot_confusion_matrix(yte, ypred)\n",
        "    plt.show()\n",
        "\n",
        "# Read the data\n",
        "if not os.path.isfile('./xtr.npy') or \\\n",
        "    not os.path.isfile('./xte.npy') or \\\n",
        "    not os.path.isfile('./ytr.npy') or \\\n",
        "    not os.path.isfile('./yte.npy'):\n",
        "    xtr,xte,ytr,yte = getEmbeddings(\"C:/Users/abinaya/OneDrive/Documents/train.csv\")\n",
        "    np.save('./xtr', xtr)\n",
        "    np.save('./xte', xte)\n",
        "    np.save('./ytr', ytr)\n",
        "    np.save('./yte', yte)\n",
        "\n",
        "xtr = np.load('./xtr.npy')\n",
        "xte = np.load('./xte.npy')\n",
        "ytr = np.load('./ytr.npy')\n",
        "yte = np.load('./yte.npy')\n",
        "\n",
        "\n",
        "def baseline_model():\n",
        "    '''Neural network with 3 hidden layers'''\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_dim=300, activation='relu', kernel_initializer='normal'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(256, activation='relu', kernel_initializer='normal'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(80, activation='relu', kernel_initializer='normal'))\n",
        "    model.add(Dense(2, activation=\"softmax\", kernel_initializer='normal'))\n",
        "\n",
        "    # gradient descent\n",
        "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "    # configure the learning process of the model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train the model\n",
        "model = baseline_model()\n",
        "model.summary()\n",
        "x_train, x_test, y_train, y_test = train_test_split(xtr, ytr, test_size=0.2, random_state=42)\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(y_train)\n",
        "encoded_y = np_utils.to_categorical((label_encoder.transform(y_train)))\n",
        "label_encoder.fit(y_test)\n",
        "encoded_y_test = np_utils.to_categorical((label_encoder.transform(y_test)))\n",
        "estimator = model.fit(x_train, encoded_y, epochs=20, batch_size=64)\n",
        "print(\"Model Trained!\")\n",
        "score = model.evaluate(x_test, encoded_y_test)\n",
        "print(\"\")\n",
        "print(\"Accuracy = \" + format(score[1]*100, '.2f') + \"%\")\n",
        "\n",
        "probabs = model.predict_proba(x_test)\n",
        "y_pred = np.argmax(probabs, axis=1)\n",
        "\n",
        "# Draw the confusion matrix\n",
        "plot_cmat(y_test, y_pred)"
      ],
      "metadata": {
        "id": "0Ts3MHgN22gE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Neural Network Model(Tensor Flow):"
      ],
      "metadata": {
        "id": "v05jViqp4Wy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy Of The Model:83.58%"
      ],
      "metadata": {
        "id": "wCo6jVt64msx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Fake News Detection Analysis\n",
        "The Neural Network- Tensor Flow\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from getEmbeddings import getEmbeddings\n",
        "import matplotlib.pyplot as plt\n",
        "import scikitplot.plotters as skplt\n",
        "import os.path\n",
        "\n",
        "IN_DIM = 300\n",
        "CLASS_NUM = 2\n",
        "LEARN_RATE = 0.0001\n",
        "TRAIN_STEP = 20000\n",
        "tensorflow_tmp = \"tmp_tensorflow\"\n",
        "\n",
        "\n",
        "def plot_cmat(yte, ypred):\n",
        "    '''Plotting confusion matrix'''\n",
        "    skplt.plot_confusion_matrix(yte,ypred)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def dummy_input_fn():\n",
        "    return np.array([1.0] * IN_DIM)\n",
        "\n",
        "\n",
        "def model_fn(features, labels, mode):\n",
        "    \"\"\"The model function for tf.Estimator\"\"\"\n",
        "    # Input layer\n",
        "    input_layer = tf.reshape(features[\"x\"], [-1, IN_DIM])\n",
        "    # Dense layer1\n",
        "    dense1 = tf.layers.dense(inputs=input_layer, units=300, \\\n",
        "        activation=tf.nn.relu)\n",
        "    # Dropout layer1\n",
        "    dropout1 = tf.layers.dropout(inputs=dense1, rate=0.4, \\\n",
        "        training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
        "    # Dense layer2\n",
        "    dense2 = tf.layers.dense(inputs=dropout1, units=300, \\\n",
        "        activation=tf.nn.relu)\n",
        "    # Dropout layer2\n",
        "    dropout2 = tf.layers.dropout(inputs=dense2, rate=0.4, \\\n",
        "        training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
        "    # Dense layer3\n",
        "    dense3 = tf.layers.dense(inputs=dropout2, units=300, \\\n",
        "        activation=tf.nn.relu)\n",
        "    # Dropout layer3\n",
        "    dropout3 = tf.layers.dropout(inputs=dense3, rate=0.4, \\\n",
        "        training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
        "    # Logits layer\n",
        "    logits = tf.layers.dense(inputs=dropout3, units=CLASS_NUM)\n",
        "\n",
        "    # prediction result in PREDICT and EVAL phases\n",
        "    predictions = {\n",
        "        # Class id\n",
        "        \"classes\": tf.argmax(input=logits, axis=1),\n",
        "        # Probabilities\n",
        "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
        "    }\n",
        "\n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
        "\n",
        "    # Calculate Loss for TRAIN and EVAL\n",
        "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
        "\n",
        "    # Configure the training Op\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARN_RATE)\n",
        "        train_op = optimizer.minimize(\\\n",
        "            loss=loss, global_step=tf.train.get_global_step())\n",
        "        return tf.estimator.EstimatorSpec(\\\n",
        "            mode=mode, loss=loss, train_op=train_op)\n",
        "\n",
        "    # Add evaluation metrics\n",
        "    eval_metric_ops = {\n",
        "        \"accuracy\": tf.metrics.accuracy(\\\n",
        "            labels=labels, predictions=predictions[\"classes\"])\n",
        "    }\n",
        "    return tf.estimator.EstimatorSpec(\\\n",
        "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Get the training and testing data from getEmbeddings\n",
        "    if not os.path.isfile('./xtr.npy') or \\\n",
        "        not os.path.isfile('./xte.npy') or \\\n",
        "        not os.path.isfile('./ytr.npy') or \\\n",
        "        not os.path.isfile('./yte.npy'):\n",
        "        xtr,xte,ytr,yte = getEmbeddings(\"C:/Users/abinaya/OneDrive/Documents/train.csv\")\n",
        "        np.save('./xtr', xtr)\n",
        "        np.save('./xte', xte)\n",
        "        np.save('./ytr', ytr)\n",
        "        np.save('./yte', yte)\n",
        "    # Read the Doc2Vec data\n",
        "    train_data = np.load('./xtr.npy')\n",
        "    eval_data = np.load('./xte.npy')\n",
        "    train_labels = np.load('./ytr.npy')\n",
        "    eval_labels = np.load('./yte.npy')\n",
        "    train_labels = train_labels.reshape((-1, 1)).astype(np.int32)\n",
        "    eval_labels = eval_labels.reshape((-1, 1)).astype(np.int32)\n",
        "\n",
        "    # Create the Estimator\n",
        "    classifier = \\\n",
        "        tf.estimator.Estimator(model_fn=model_fn, model_dir=tensorflow_tmp)\n",
        "\n",
        "    # Setup logging hook for prediction\n",
        "    tf.logging.set_verbosity(tf.logging.INFO)\n",
        "    tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
        "    logging_hook = tf.train.LoggingTensorHook(\n",
        "        tensors=tensors_to_log, every_n_iter=200)\n",
        "\n",
        "    # Train the model\n",
        "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "        x={\"x\": train_data},\n",
        "        y=train_labels,\n",
        "        batch_size=50,\n",
        "        num_epochs=None,\n",
        "        shuffle=True)\n",
        "    classifier.train(\n",
        "        input_fn=train_input_fn,\n",
        "        steps=TRAIN_STEP,\n",
        "        hooks=[logging_hook])\n",
        "\n",
        "    # Evaluate the model and print results\n",
        "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "        x={\"x\": eval_data},\n",
        "        y=eval_labels,\n",
        "        num_epochs=1,\n",
        "        shuffle=False)\n",
        "    eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
        "    print(eval_results)\n",
        "\n",
        "    # Draw the confusion matrix\n",
        "    predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "        x={\"x\": eval_data},\n",
        "        num_epochs=1,\n",
        "        shuffle=False)\n",
        "    predict_results = classifier.predict(input_fn=predict_input_fn)\n",
        "    predict_labels = [label[\"classes\"] for label in predict_results]\n",
        "    plot_cmat(eval_labels, predict_labels)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "TXSVVxAn3K2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM (Long Short-Term Memory) Model:\n"
      ],
      "metadata": {
        "id": "8q4MfvWI3lVO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy Of The Model: 96.76%"
      ],
      "metadata": {
        "id": "XMveFm3M4Q-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Fake News Detection Analysis\n",
        "LSTM model\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from collections import Counter\n",
        "import os\n",
        "import getEmbeddings\n",
        "import matplotlib.pyplot as plt\n",
        "import scikitplot.plotters as skplt\n",
        "\n",
        "\n",
        "top_words = 5000\n",
        "epoch_num = 5\n",
        "batch_size = 64\n",
        "\n",
        "def plot_cmat(yte, ypred):\n",
        "    '''Plotting confusion matrix'''\n",
        "    skplt.plot_confusion_matrix(yte, ypred)\n",
        "    plt.show()\n",
        "\n",
        "# Read the text data\n",
        "if not os.path.isfile('./xtr_shuffled.npy') or \\\n",
        "    not os.path.isfile('./xte_shuffled.npy') or \\\n",
        "    not os.path.isfile('./ytr_shuffled.npy') or \\\n",
        "    not os.path.isfile('./yte_shuffled.npy'):\n",
        "    getEmbeddings.clean_data()\n",
        "\n",
        "\n",
        "xtr = np.load('./xtr_shuffled.npy')\n",
        "xte = np.load('./xte_shuffled.npy')\n",
        "y_train = np.load('./ytr_shuffled.npy')\n",
        "y_test = np.load('./yte_shuffled.npy')\n",
        "\n",
        "cnt = Counter()\n",
        "x_train = []\n",
        "for x in xtr:\n",
        "    x_train.append(x.split())\n",
        "    for word in x_train[-1]:\n",
        "        cnt[word] += 1\n",
        "\n",
        "# Storing most common words\n",
        "most_common = cnt.most_common(top_words + 1)\n",
        "word_bank = {}\n",
        "id_num = 1\n",
        "for word, freq in most_common:\n",
        "    word_bank[word] = id_num\n",
        "    id_num += 1\n",
        "\n",
        "# Encode the sentences\n",
        "for news in x_train:\n",
        "    i = 0\n",
        "    while i < len(news):\n",
        "        if news[i] in word_bank:\n",
        "            news[i] = word_bank[news[i]]\n",
        "            i += 1\n",
        "        else:\n",
        "            del news[i]\n",
        "\n",
        "y_train = list(y_train)\n",
        "y_test = list(y_test)\n",
        "\n",
        "# Delete the short news\n",
        "i = 0\n",
        "while i < len(x_train):\n",
        "    if len(x_train[i]) > 10:\n",
        "        i += 1\n",
        "    else:\n",
        "        del x_train[i]\n",
        "        del y_train[i]\n",
        "\n",
        "# Generating test data\n",
        "x_test = []\n",
        "for x in xte:\n",
        "    x_test.append(x.split())\n",
        "\n",
        "# Encode the sentences\n",
        "for news in x_test:\n",
        "    i = 0\n",
        "    while i < len(news):\n",
        "        if news[i] in word_bank:\n",
        "            news[i] = word_bank[news[i]]\n",
        "            i += 1\n",
        "        else:\n",
        "            del news[i]\n",
        "\n",
        "# Truncate and pad input sequences\n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(x_test, maxlen=max_review_length)\n",
        "\n",
        "# Convert to numpy arrays\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Create the model\n",
        "embedding_vecor_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words+2, embedding_vecor_length, input_length=max_review_length))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epoch_num, batch_size=batch_size)\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy= %.2f%%\" % (scores[1]*100))\n",
        "\n",
        "# Draw the confusion matrix\n",
        "y_pred = model.predict_classes(X_test)\n",
        "plot_cmat(y_test, y_pred)"
      ],
      "metadata": {
        "id": "HDbrC6i53bCk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}